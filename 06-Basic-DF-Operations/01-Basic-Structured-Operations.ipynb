{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Basic Structured Operations\n\nVariable `data` shows where data is located. Modify it as needed"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "data = \"gs://is843/notebooks/jupyter/data/\""}, {"cell_type": "markdown", "metadata": {}, "source": "We first load a sample data that we worked with in the previous class, but its JSON corresponding format. Source of the data is flight data from the United States Bureau of Transportation statistics from 2015:"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read.format(\"json\").load(data + \"flight-data/json/2015-summary.json\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## Schemas\nA schema defines the column names and types of a DataFrame. We can either let a data source define the schema (called schema-on-read) or we can define it explicitly ourselves.\n\nIn the cell above we have used the schema-on-read option. Let\u2019s take a look at the schema on our current DataFrame:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "Now let's have a look at its first row:"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": "df.first()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can also use `.schema` method to get the schema recipe that Spark is planning to use to load our file. We could take this recipe as a starting point to define our own schema:"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "spark.read.format(\"json\").load(data + \"flight-data/json/2015-summary.json\").schema"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"data": {"text/plain": "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "df.first()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Explicit Schema Definition\n\nA schema is a `StructType` made up of a number of fields, `StructFields`, that have a name, type, and a Boolean flag which specifies whether that column can contain missing or null values.\n\nThe following shows how to create and enforce a specific schema on a DataFrame:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])\n"}], "source": "from pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmyManualSchema = StructType([\n  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n  StructField(\"count\", LongType(), True)\n])\n\nprint(myManualSchema)"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": "df = spark.read.format(\"json\").schema(myManualSchema)\\\n  .load(data + \"flight-data/json/2015-summary.json\")"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Columns and Expressions\n\nColumns in Spark are similar to columns in a spreadsheet, R dataframe, or pandas DataFrame. You can select, manipulate, and remove columns from DataFrames and these operations are represented as expressions.\n\nYou cannot manipulate an individual column outside the context of a DataFrame; you must use Spark transformations within a DataFrame to modify the contents of a column.\n\n### Columns\n\nThere are a lot of different ways to construct and refer to columns but the simplest way is by using the `col` function. To use this function, you pass in a column name:"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"data": {"text/plain": "Column<'someColumnName'>"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.functions import col\ncol(\"someColumnName\")"}, {"cell_type": "markdown", "metadata": {}, "source": "This column might or might not exist in our DataFrames. Columns are not *resolved* until we compare the column names with those we are maintaining in the *catalog*. Column and table resolution happens in the *analyzer* phase. \n\n### EXPLICIT COLUMN REFERENCES\n\nIf you need to refer to a specific DataFrame\u2019s column, you can use one of the following methods on the specific DataFrame."}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"data": {"text/plain": "Column<'ORIGIN_COUNTRY_NAME'>"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "df.ORIGIN_COUNTRY_NAME"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"data": {"text/plain": "Column<'ORIGIN_COUNTRY_NAME'>"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "df[\"ORIGIN_COUNTRY_NAME\"]"}, {"cell_type": "markdown", "metadata": {}, "source": "## Expressions\n\nAn *expression* is a set of transformations on one or more values in a record in a DataFrame. Think of it like a function that takes as input one or more column names, resolves them, and then potentially applies more expressions to create a single value for each record in the dataset.\n\nIn the simplest case, an expression, created via the expr function, is just a DataFrame column reference. In the simplest case, `expr(\"someCol\")` is equivalent to `col(\"someCol\")`.\n\n### COLUMNS AS EXPRESSIONS\n\nThe `expr` function can actually parse transformations and column references from a string and can subsequently be passed into further transformations. Let\u2019s look at some examples:\n\n`expr(\"someCol - 5\")` is the same transformation as performing `col(\"someCol\") - 5`, or even `expr(\"someCol\") - 5`.\n\nThat\u2019s because Spark compiles these to a logical tree specifying the order of operations. This might be a bit confusing at first, but remember a couple of key points:\n\n* Columns are just expressions.\n\n* Columns and transformations of those columns compile to the same logical plan as parsed expressions.\n\nLet\u2019s ground this with an example:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"data": {"text/plain": "Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"}, "execution_count": 13, "metadata": {}, "output_type": "execute_result"}], "source": "(((col(\"someCol\") + 5) * 200) - 6) < col(\"otherCol\")"}, {"cell_type": "markdown", "metadata": {}, "source": "<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/05-01-logical-tree.png?raw=true\" width=\"700\" align=\"center\"/>\n\nThis might look familiar because it\u2019s a directed acyclic graph. This graph is represented equivalently by the following code, using `expr()` function:"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"data": {"text/plain": "Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"}, "execution_count": 14, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql.functions import expr\nexpr(\"(((someCol + 5) * 200) - 6) < otherCol\")"}, {"cell_type": "markdown", "metadata": {}, "source": "This is an extremely important point to reinforce. Notice how the previous expression is actually valid SQL code, as well, just like you might put in a SELECT statement? That\u2019s because this SQL expression and the previous DataFrame code compile to the same underlying logical tree prior to execution. This means that you can write your expressions as DataFrame code or as SQL expressions and get the exact same performance characteristics.\n\n### ACCESSING A DATAFRAME\u2019S COLUMNS\n\nSometimes, you\u2019ll need to see a DataFrame\u2019s columns, which you can do by using something like printSchema; however, if you want to programmatically access columns, you can use the columns property to see all columns on a DataFrame:"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"data": {"text/plain": "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"}, "execution_count": 15, "metadata": {}, "output_type": "execute_result"}], "source": "df.columns"}, {"cell_type": "markdown", "metadata": {}, "source": "## Records and Rows\n\nIn Spark, each row in a DataFrame is a single record. Spark represents this record as an object of type Row. Spark manipulates Row objects using column expressions in order to produce usable values.\n\nLet\u2019s see a row by calling `head()` on our DataFrame (equivalent to `first()` but with the functionality to print more Rows if n is specified):"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"data": {"text/plain": "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "df.head()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Creating Rows\n\nYou can create rows by manually instantiating a Row object with the values that belong in each column. It\u2019s important to note that only DataFrames have schemas. Rows themselves do not have schemas. This means that if you create a Row manually, you must specify the values in the same order as the schema of the DataFrame to which they might be appended (we will see this when we discuss creating DataFrames):"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": "from pyspark.sql import Row\nmyRow = Row(\"Hello\", None, 1, False)"}, {"cell_type": "markdown", "metadata": {}, "source": "We can access the Row elements by familiar Python list syntax:"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"data": {"text/plain": "'Hello'"}, "execution_count": 18, "metadata": {}, "output_type": "execute_result"}], "source": "myRow[0]"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"data": {"text/plain": "('Hello', None, 1)"}, "execution_count": 19, "metadata": {}, "output_type": "execute_result"}], "source": "myRow[0:3]"}, {"cell_type": "markdown", "metadata": {}, "source": "## DataFrame Transformations\n\nNow that we briefly defined the core parts of a DataFrame, we will move onto manipulating DataFrames. When working with individual DataFrames there are some fundamental objectives. These break down into several core operations, as shown in the figure below:\n\n* We can add rows or columns\n* We can remove rows or columns\n* We can transform a row into a column (or vice versa)\n* We can change the order of rows based on the values in columns\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/05-01-Different-kinds-of-transformation.png?raw=true\" width=\"700\" align=\"center\"/>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Creating DataFrames\n\nWe will continue using the `df` DataFrame that we loaded early from the flight data. We will also register this as a temporary view so that we can query it with SQL and show off basic transformations in SQL, as well:"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": "df.createOrReplaceTempView(\"dfTable\")"}, {"cell_type": "markdown", "metadata": {}, "source": "We can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame."}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 6:===================>                                       (1 + 2) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+----+-----+\n| some| col|names|\n+-----+----+-----+\n|Hello|null|    1|\n+-----+----+-----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import Row\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmyManualSchema = StructType([\n  StructField(\"some\", StringType(), True),\n  StructField(\"col\", StringType(), True),\n  StructField(\"names\", LongType(), False)\n])\n\nmyRow = Row(\"Hello\", None, 1)\nmyDf = spark.createDataFrame([myRow], myManualSchema)\nmyDf.show()"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- some: string (nullable = true)\n |-- col: string (nullable = true)\n |-- names: long (nullable = false)\n\n"}], "source": "myDf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "### select and selectExpr\n\n`select` and `selectExpr` allow you to do the DataFrame equivalent of SQL queries on a table of data:"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n"}], "source": "df.select(\"DEST_COUNTRY_NAME\").show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "And its SQL equivalence:"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\n\n"}], "source": "spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "You can select multiple columns by using the same style of query, just add more column name strings to your select method call:"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n+-----------------+-------------------+\nonly showing top 2 rows\n\n"}], "source": "df.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n|    United States|            Romania|\n|    United States|            Croatia|\n+-----------------+-------------------+\n\n"}], "source": "spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2\n\"\"\").show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "**Note:** You can refer to columns in a number of different ways; all you need to keep in mind is that you can use them interchangeably:"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-----------------+-----------------+\n|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-----------------+-----------------+-----------------+\n|    United States|    United States|    United States|\n|    United States|    United States|    United States|\n+-----------------+-----------------+-----------------+\nonly showing top 2 rows\n\n"}], "source": "from pyspark.sql.functions import expr, col\ndf.select(\n    expr(\"DEST_COUNTRY_NAME\"),\n    col(\"DEST_COUNTRY_NAME\"),\n    \"DEST_COUNTRY_NAME\"\n).show(2)"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[DEST_COUNTRY_NAME: string, DEST_COUNTRY_NAME: string]"}, "execution_count": 28, "metadata": {}, "output_type": "execute_result"}], "source": "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\")"}, {"cell_type": "markdown", "metadata": {}, "source": "### RENAMING COLUMNS"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+\n|  destination|\n+-------------+\n|United States|\n|United States|\n+-------------+\nonly showing top 2 rows\n\n"}], "source": "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "This changes the column name to \u201cdestination.\u201d You can further manipulate the result of your expression as another expression:"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+\n|DEST_COUNTRY_NAME|\n+-----------------+\n|    United States|\n|    United States|\n+-----------------+\nonly showing top 2 rows\n\n"}], "source": "df.select(expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")).show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "Because select followed by a series of expr is such a common pattern, Spark has a shorthand for doing this efficiently: selectExpr. This is probably the most convenient interface for everyday use:"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+-----------------+\n|newColumnName|DEST_COUNTRY_NAME|\n+-------------+-----------------+\n|United States|    United States|\n|United States|    United States|\n+-------------+-----------------+\nonly showing top 2 rows\n\n"}], "source": "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "We can add any valid non-aggregating SQL statement, and as long as the columns resolve, it will be valid! Here\u2019s a simple example that adds a new column withinCountry to our DataFrame that specifies whether the destination and origin are the same:"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"}], "source": "df.selectExpr(\n  \"*\", # all original columns\n  \"(DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n  .show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "And its SQL equivalence:"}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\n\n"}], "source": "spark.sql(\"\"\"\nSELECT *, (DEST_COUNTRY_NAME == ORIGIN_COUNTRY_NAME) as withinCountry\nFROM dfTable\nLIMIT 2\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "With select expression, we can also specify aggregations over the entire DataFrame by taking advantage of the functions that we have. These look just like what we have been showing so far:"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+---------------------------------+\n| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n|1770.765625|                              132|\n+-----------+---------------------------------+\n\n"}], "source": "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Converting to Spark Types (Literals)\n\nSometimes, we need to pass explicit values into Spark that are just a value (rather than a new column). This might be a constant value or something we\u2019ll need to compare to later on. The way we do this is through literals. This is basically a translation from a given programming language\u2019s literal value to one that Spark understands. \n\n**Literals are expressions** and you can use them in the same way:"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\nonly showing top 2 rows\n\n"}], "source": "from pyspark.sql.functions import lit\ndf.select(expr(\"*\"), lit(1).alias(\"One\")).show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL, literals are just the specific value:"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+---+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n+-----------------+-------------------+-----+---+\n|    United States|            Romania|   15|  1|\n|    United States|            Croatia|    1|  1|\n+-----------------+-------------------+-----+---+\n\n"}], "source": "spark.sql(\"\"\"\nSELECT *, 1 as One FROM dfTable LIMIT 2\n\"\"\").show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Adding Columns\n\nThere\u2019s also a more formal way of adding a new column to a DataFrame, and that\u2019s by using the withColumn method on our DataFrame. For example, let\u2019s add a column that just adds the number one as a column:"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+---------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n+-----------------+-------------------+-----+---------+\n|    United States|            Romania|   15|        1|\n|    United States|            Croatia|    1|        1|\n+-----------------+-------------------+-----+---------+\nonly showing top 2 rows\n\n"}], "source": "df.withColumn(\"numberOne\", lit(1)).show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "Let\u2019s do something a bit more interesting and make it an actual expression. In the next example, we\u2019ll set a Boolean flag for when the origin country is the same as the destination country:"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+-------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n|    United States|            Romania|   15|        false|\n|    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n"}], "source": "df.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")).show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "Notice that the withColumn function takes two arguments: the column name and the expression that will create the value for that given row in the DataFrame."}, {"cell_type": "markdown", "metadata": {}, "source": "### RENAMING DATAFRAME COLUMNS"}, {"cell_type": "code", "execution_count": 39, "metadata": {}, "outputs": [{"data": {"text/plain": "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'Destination']"}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": "df.withColumn(\"Destination\", expr(\"DEST_COUNTRY_NAME\")).columns"}, {"cell_type": "code", "execution_count": 40, "metadata": {}, "outputs": [{"data": {"text/plain": "['dest', 'orig', 'count']"}, "execution_count": 40, "metadata": {}, "output_type": "execute_result"}], "source": "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\")\\\n  .withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"orig\")\\\n  .columns"}, {"cell_type": "markdown", "metadata": {}, "source": "### Reserved Characters and Keywords\n\nOne thing that you might come across is reserved characters like spaces or dashes in column names. Handling these means escaping column names appropriately. We already have seen this in BigQyery syntax. In Spark, we do this by using backtick ( \\` ) characters. Let\u2019s use withColumn, which you just learned about to create a column with reserved characters. We\u2019ll show two examples\u2014in the one shown here, we don\u2019t need escape characters, but in the next one, we do:"}, {"cell_type": "code", "execution_count": 41, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+---------------------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n+-----------------+-------------------+-----+---------------------+\n|    United States|            Romania|   15|              Romania|\n|    United States|            Croatia|    1|              Croatia|\n+-----------------+-------------------+-----+---------------------+\nonly showing top 2 rows\n\n"}], "source": "dfWithLongColName = df.withColumn(\n    \"This Long Column-Name\",\n    expr(\"ORIGIN_COUNTRY_NAME\"))\n\ndfWithLongColName.show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "We don\u2019t need escape characters here because the first argument to withColumn is just a string for the new column name. In this example, however, we need to use backticks because we\u2019re referencing a column in an expression:"}, {"cell_type": "code", "execution_count": 42, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------------+-------+\n|This Long Column-Name|new col|\n+---------------------+-------+\n|              Romania|Romania|\n|              Croatia|Croatia|\n+---------------------+-------+\nonly showing top 2 rows\n\n"}], "source": "dfWithLongColName.selectExpr(\n    \"`This Long Column-Name`\",\n    \"`This Long Column-Name` as `new col`\")\\\n  .show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "And its SQL equivalence:"}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------------+-------+\n|This Long Column-Name|new col|\n+---------------------+-------+\n|              Romania|Romania|\n|              Croatia|Croatia|\n+---------------------+-------+\n\n"}], "source": "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")\n\nspark.sql(\"\"\"\nSELECT `This Long Column-Name`, `This Long Column-Name` as `new col`\nFROM dfTableLong LIMIT 2\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can refer to columns with reserved characters (and not escape them) if we\u2019re doing an explicit string-to-column reference, which is interpreted as a literal instead of an expression. We only need to escape expressions that use reserved characters or keywords. The following two examples both result in the same DataFrame:"}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [{"data": {"text/plain": "['This Long Column-Name']"}, "execution_count": 44, "metadata": {}, "output_type": "execute_result"}], "source": "dfWithLongColName.select(col(\"This Long Column-Name\")).columns"}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [{"data": {"text/plain": "['This Long Column-Name']"}, "execution_count": 45, "metadata": {}, "output_type": "execute_result"}], "source": "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"}, {"cell_type": "markdown", "metadata": {}, "source": "### Removing Columns\n\nNow that we\u2019ve created this column, let\u2019s take a look at how we can remove columns from DataFrames. You likely already noticed that we can do this by using select. However, there is also a dedicated method called drop:"}, {"cell_type": "code", "execution_count": 46, "metadata": {}, "outputs": [{"data": {"text/plain": "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count', 'This Long Column-Name']"}, "execution_count": 46, "metadata": {}, "output_type": "execute_result"}], "source": "dfWithLongColName.columns"}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [{"data": {"text/plain": "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"}, "execution_count": 47, "metadata": {}, "output_type": "execute_result"}], "source": "dfWithLongColName.drop(\"This Long Column-Name\").columns"}, {"cell_type": "markdown", "metadata": {}, "source": "We can drop multiple columns by passing in multiple columns as arguments:"}, {"cell_type": "code", "execution_count": 48, "metadata": {}, "outputs": [{"data": {"text/plain": "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME']"}, "execution_count": 48, "metadata": {}, "output_type": "execute_result"}], "source": "dfWithLongColName.drop(\"This Long Column-Name\", \"count\").columns"}, {"cell_type": "markdown", "metadata": {}, "source": "### Changing a Column\u2019s Type (cast)\n\nSometimes, we might need to convert from one type to another; for example, if we have a set of StringType that should be integers. We can convert columns from one type to another by casting the column from one type to another. For instance, let\u2019s convert our count column from an integer to a type Long:"}, {"cell_type": "code", "execution_count": 49, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n |-- count2: long (nullable = true)\n\n"}], "source": "df.withColumn(\"count2\", col(\"count\").cast(\"long\")).printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "And its SQL equivalence:"}, {"cell_type": "code", "execution_count": 51, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- DEST_COUNTRY_NAME: string (nullable = true)\n |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n |-- count: long (nullable = true)\n |-- count2: long (nullable = true)\n\n"}], "source": "spark.sql(\"\"\"\nSELECT *, cast(count as long) AS count2 FROM dfTable LIMIT 2\n\"\"\").printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Filtering Rows\n\nTo filter rows, we create an expression that evaluates to true or false. You then filter out the rows with an expression that is equal to false. The most common way to do this with DataFrames is to create either an expression as a String or build an expression by using a set of column manipulations. There are two methods to perform this operation: you can use `where` or `filter` and they both will perform the same operation and accept the same argument types when used with DataFrames. We will stick to `where` because of its familiarity to SQL; however, `filter` is valid as well."}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|          Singapore|    1|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"}], "source": "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n  .show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "The following filters are equivalent, and the results are the same:"}, {"cell_type": "code", "execution_count": 53, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"}], "source": "df.where(col(\"count\") < 2).show(2)\ndf.where(\"count < 2\").show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "And their SQL equivalence:"}, {"cell_type": "code", "execution_count": 54, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Croatia|    1|\n|    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\n\n"}], "source": "spark.sql(\"\"\"\nSELECT * FROM dfTable WHERE count < 2 \nLIMIT 2\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Instinctually, you might want to put multiple filters into the same expression. Although this is possible, it is not always useful, because Spark automatically performs all filtering operations at the same time regardless of the filter ordering. This means that if you want to specify multiple AND filters, just chain them sequentially and let Spark handle the rest:"}, {"cell_type": "code", "execution_count": 55, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|          Singapore|    1|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n"}], "source": "df.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n  .show(2)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Getting Unique Rows\n\nThe way we do this is by using the distinct method on a DataFrame, which allows us to deduplicate any rows that are in that DataFrame. For instance, let\u2019s get the unique origins in our dataset. This, of course, is a transformation that will return a new DataFrame with only unique rows:"}, {"cell_type": "code", "execution_count": 56, "metadata": {}, "outputs": [{"data": {"text/plain": "125"}, "execution_count": 56, "metadata": {}, "output_type": "execute_result"}], "source": "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()\n"}, {"cell_type": "markdown", "metadata": {}, "source": "We can also get the unique values for multiple fields:"}, {"cell_type": "code", "execution_count": 57, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------+-----------------+\n|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|\n+-------------------+-----------------+\n|            Romania|    United States|\n|            Croatia|    United States|\n|            Ireland|    United States|\n|      United States|            Egypt|\n|              India|    United States|\n+-------------------+-----------------+\nonly showing top 5 rows\n\n"}], "source": "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Random Samples\n\nSometimes, you might just want to sample some random records from your DataFrame. You can do this by using the sample method on a DataFrame, which makes it possible for you to specify a fraction of rows to extract from a DataFrame and whether you\u2019d like to sample with or without replacement:"}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|       United States|       Sint Maarten|  325|\n|               Italy|      United States|  382|\n|            Pakistan|      United States|   12|\n|       United States|           Anguilla|   38|\n|              Sweden|      United States|  118|\n|       United States|            Estonia|    1|\n|       United States|           Pakistan|   12|\n|              Kuwait|      United States|   32|\n|Federated States ...|      United States|   69|\n|             Finland|      United States|   26|\n|British Virgin Is...|      United States|  107|\n|               Spain|      United States|  420|\n|             Tunisia|      United States|    3|\n|               Niger|      United States|    2|\n+--------------------+-------------------+-----+\n\n"}], "source": "df.sample(withReplacement = False, fraction = 0.05, seed = 8).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Random Splits\n\nRandom splits can be helpful when you need to break up your DataFrame into a random \u201csplits\u201d of the original DataFrame. This is often used with machine learning algorithms to create training, validation, and test sets. In this next example, we\u2019ll split our DataFrame into two different DataFrames by setting the weights by which we will split the DataFrame (these are the arguments to the function). Because this method is designed to be randomized, we will also specify a seed (just replace seed with a number of your choosing in the code block). It\u2019s important to note that if you don\u2019t specify a proportion for each DataFrame that adds up to one, they will be normalized so that they do:"}, {"cell_type": "code", "execution_count": 59, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "209\n47\n"}], "source": "dataFrames = df.randomSplit([0.8, 0.2], seed = 9)\nprint(dataFrames[0].count())\nprint(dataFrames[1].count())"}, {"cell_type": "markdown", "metadata": {}, "source": "### Concatenating and Appending Rows (Union)\n\nTo union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail.\n\nWARNING: Unions are currently performed based on location, not on the schema. This means that columns will not automatically line up the way you think they might."}, {"cell_type": "code", "execution_count": 60, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|      New Country|      Other Country|    5|\n|    New Country 2|    Other Country 3|    1|\n+-----------------+-------------------+-----+\n\n"}], "source": "from pyspark.sql import Row\nschema = df.schema\nnewRows = [\n  Row(\"New Country\", \"Other Country\", 5),\n  Row(\"New Country 2\", \"Other Country 3\", 1)\n]\nparallelizedRows = spark.sparkContext.parallelize(newRows)\nnewDF = spark.createDataFrame(parallelizedRows, schema)\nnewDF.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Let's create a smaller df to perform the union on:"}, {"cell_type": "code", "execution_count": 61, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n+-----------------+-------------------+-----+\n\n"}], "source": "dfSmall = df.limit(3)\ndfSmall.show()"}, {"cell_type": "code", "execution_count": 62, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|      New Country|      Other Country|    5|\n|    New Country 2|    Other Country 3|    1|\n+-----------------+-------------------+-----+\n\n"}], "source": "dfSmall.union(newDF).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Sorting Rows\n\nWhen we sort the values in a DataFrame, we always want to sort with either the largest or smallest values at the top of a DataFrame. There are two equivalent operations to do this sort and orderBy that work the exact same way. They accept both column expressions and strings as well as multiple columns. The default is to sort in ascending order:"}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------+-------------------+-----+\n|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n|               Malta|      United States|    1|\n|Saint Vincent and...|      United States|    1|\n|       United States|            Croatia|    1|\n|       United States|          Gibraltar|    1|\n|       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d'Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|     Burkina Faso|      United States|    1|\n|    Cote d'Ivoire|      United States|    1|\n|           Cyprus|      United States|    1|\n|         Djibouti|      United States|    1|\n|        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n"}], "source": "df.sort(\"count\").show(5)\ndf.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\ndf.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "To more explicitly specify sort direction, you need to use the asc and desc functions if operating on a column. These allow you to specify the order in which a given column should be sorted:"}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n+-----------------+-------------------+------+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|           Zambia|      United States|    1|\n|        Venezuela|      United States|  290|\n|          Uruguay|      United States|   43|\n|    United States|             Angola|   13|\n|    United States|           Anguilla|   38|\n|    United States|Antigua and Barbuda|  117|\n+-----------------+-------------------+-----+\nonly showing top 6 rows\n\n"}], "source": "from pyspark.sql.functions import desc, asc\ndf.orderBy(col(\"count\").desc()).show(2)\ndf.orderBy(col(\"DEST_COUNTRY_NAME\").desc(), col(\"ORIGIN_COUNTRY_NAME\").asc()).show(6)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Limit\n\nOftentimes, you might want to restrict what you extract from a DataFrame; for example, you might want just the top five of some DataFrame. You can do this by using the limit method:"}, {"cell_type": "code", "execution_count": 65, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n"}], "source": "df.limit(5).show()"}, {"cell_type": "code", "execution_count": 66, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n+-----------------+-------------------+-----+\n\n"}], "source": "spark.sql(\"SELECT * FROM dfTable LIMIT 5\").show()"}, {"cell_type": "code", "execution_count": 67, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+------+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n|    United States|      United States|370002|\n|    United States|             Canada|  8483|\n|           Canada|      United States|  8399|\n|    United States|             Mexico|  7187|\n|           Mexico|      United States|  7140|\n|   United Kingdom|      United States|  2025|\n+-----------------+-------------------+------+\n\n"}], "source": "df.orderBy(col(\"count\").desc()).limit(6).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Repartition and Coalesce\n\nAn important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partitioning scheme and the number of partitions.\n\nIf you know that you\u2019re going to be filtering by a certain column often, it can be worth repartitioning based on that column (You can optionally specify the number of partitions you would like):"}, {"cell_type": "code", "execution_count": 68, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"}, "execution_count": 68, "metadata": {}, "output_type": "execute_result"}], "source": "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "### Collecting Rows to the Driver\n\nAs discussed in previous chapters, Spark maintains the state of the cluster in the driver. There are times when you\u2019ll want to collect some of your data to the driver in order to manipulate it on your local machine.\n\nThus far, we did not explicitly define this operation. However, we used several different methods for doing so that are effectively all the same. `collect` gets all data from the entire DataFrame, `take` selects the first N rows, and `show` prints out a number of rows nicely."}, {"cell_type": "code", "execution_count": 69, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"}, "execution_count": 69, "metadata": {}, "output_type": "execute_result"}], "source": "collectDF = df.limit(10)\ncollectDF.take(5) # take works with an Integer count"}, {"cell_type": "code", "execution_count": 70, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+-------------------+-----+\n|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n|    United States|            Romania|   15|\n|    United States|            Croatia|    1|\n|    United States|            Ireland|  344|\n|            Egypt|      United States|   15|\n|    United States|              India|   62|\n|    United States|          Singapore|    1|\n|    United States|            Grenada|   62|\n|       Costa Rica|      United States|  588|\n|          Senegal|      United States|   40|\n|          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\n\n"}], "source": "collectDF.show() # this prints it out nicely"}, {"cell_type": "code", "execution_count": 71, "metadata": {}, "outputs": [{"data": {"text/plain": "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"}, "execution_count": 71, "metadata": {}, "output_type": "execute_result"}], "source": "collectDF.collect()"}, {"cell_type": "markdown", "metadata": {}, "source": "**WARNING**\n\nAny collection of data to the driver can be a very expensive operation! If you have a large dataset and call collect, you can crash the driver."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}, "name": "05_Basic_Structured_Operations", "notebookId": 2687633780772251}, "nbformat": 4, "nbformat_minor": 4}