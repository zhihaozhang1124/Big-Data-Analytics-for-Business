{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Joins\n\nSpark applications can bring together a large number of different datasets. For this reason, joins are an essential part of nearly all Spark workloads. Spark\u2019s ability to talk to different data means that you gain the ability to tap into a variety of data sources across your company. This notebook covers not just what types of joins exist in Spark and how to use them, but also some of the basic internals so that you can think about how Spark actually goes about executing the join on the cluster. This basic knowledge can help you avoid running out of memory and tackle problems that you could not solve before.\n\n## Join Expressions\n\nA join brings together two sets of data, the left and the right, by comparing the value of one or more keys of the left and right and evaluating the result of a join expression that determines whether Spark should bring together the left set of data with the right set of data. The most common join expression, an equi-join, compares whether the specified keys in your left and right datasets are equal. If they are equal, Spark will combine the left and right datasets. The opposite is true for keys that do not match; Spark discards the rows that do not have matching keys. Spark also allows for much more sophsticated join policies in addition to equi-joins. We can even use complex types and perform something like checking whether a key exists within an array when you perform a join.\n\n## Join Types\n\nWhereas the join expression determines whether two rows should join, the join type determines what should be in the result set. There are a variety of different join types available in Spark for you to use:\n\n* Inner joins (keep rows with keys that exist in the left and right datasets)\n* Outer joins (keep rows with keys in either the left or right datasets)\n* Left outer joins (keep rows with keys in the left dataset)\n* Right outer joins (keep rows with keys in the right dataset)\n* Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n* Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\n* Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n* Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)\n\nIf you have ever interacted with a relational database system, or even an Excel spreadsheet, the concept of joining different datasets together should not be too abstract. Let\u2019s move on to showing examples of each join type. This will make it easy to understand exactly how you can apply these to your own problems. To do this, let\u2019s create some simple datasets that we can use in our examples:"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------------+----------------+---------------+\n| id|            name|graduate_program|   spark_status|\n+---+----------------+----------------+---------------+\n|  0|   Bill Chambers|               0|          [100]|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|\n|  2|Michael Armbrust|               1|     [250, 100]|\n+---+----------------+----------------+---------------+\n\n+---+-------+--------------------+-----------+\n| id| degree|          department|     school|\n+---+-------+--------------------+-----------+\n|  0|Masters|School of Informa...|UC Berkeley|\n|  2|Masters|                EECS|UC Berkeley|\n|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+-------+--------------------+-----------+\n\n"}], "source": "person = spark.createDataFrame([\n    (0, \"Bill Chambers\", 0, [100]),\n    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n\ngraduateProgram = spark.createDataFrame([\n    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n  .toDF(\"id\", \"degree\", \"department\", \"school\")\n\nperson.show()\ngraduateProgram.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Next, let\u2019s register these as tables so that we use them:"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "person.createOrReplaceTempView(\"person\")\ngraduateProgram.createOrReplaceTempView(\"graduateProgram\")"}, {"cell_type": "markdown", "metadata": {}, "source": "## Inner Joins\n\nInner joins evaluate the keys in both of the DataFrames or tables and include (and join together) only the rows that evaluate to true. In the following example, we join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "joinExpression = person[\"graduate_program\"] == graduateProgram['id']"}, {"cell_type": "markdown", "metadata": {}, "source": "Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression:"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"}], "source": "person.join(graduateProgram, joinExpression).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "We can also specify this explicitly by passing in a third parameter, the `how`:"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"}], "source": "person.join(graduateProgram, \n            on = person[\"graduate_program\"] == graduateProgram['id'],\n            how = \"inner\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "General join format:\n\n```python\nleftDF.join(rightDF, \n            on = leftDF[\"abc\"] == rightDF[\"xyz\"], \n            how = \"joinType\")\n```\n\nIn SQL\n\n```sql\nSELECT * FROM person INNER \nJOIN graduateProgram\nON person.graduate_program = graduateProgram.id\n```\n\n## Outer Joins\n\nOuter joins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. If there is no equivalent row in either the left or right DataFrame, Spark will insert null:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"}], "source": "person.join(graduateProgram, \n            on = person[\"graduate_program\"] == graduateProgram['id'],\n            how = \"outer\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT * FROM person \nFULL OUTER JOIN graduateProgram\nON graduate_program = graduateProgram.id\n```\n\n## Left Outer Joins\n\nLeft outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert null:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n| id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n|  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n|  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n|  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n\n"}], "source": "graduateProgram.join(person, \n                     on = person[\"graduate_program\"] == graduateProgram['id'],\n                     how = \"left_outer\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT * FROM graduateProgram \nLEFT OUTER JOIN person\nON person.graduate_program = graduateProgram.id\n```\n\n## Right Outer Joins\n\nRight outer joins are exactly the same as left outer joins if we replaced the order of left and right tables. It will result in a different ordering of the columns:"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|null|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"}], "source": "person.join(graduateProgram, \n            on = person[\"graduate_program\"] == graduateProgram['id'],\n            how = \"right_outer\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT * FROM person \nRIGHT OUTER JOIN graduateProgram\nON person.graduate_program = graduateProgram.id\n```\n\n## Left Semi Joins\n\nSemi joins are a bit of a departure from the other joins. They do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. If the value does exist, those rows will be kept in the result, even if there are duplicate keys in the left DataFrame. **Think of left semi joins as filters on a DataFrame**, as opposed to the function of a conventional join:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-------+--------------------+-----------+\n| id| degree|          department|     school|\n+---+-------+--------------------+-----------+\n|  0|Masters|School of Informa...|UC Berkeley|\n|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+-------+--------------------+-----------+\n\n"}], "source": "graduateProgram.join(person, \n                     on = person[\"graduate_program\"] == graduateProgram['id'],\n                     how = \"left_semi\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT * FROM gradProgram2 \nLEFT SEMI JOIN person\nON gradProgram2.id = person.graduate_program\n```\n\n## Left Anti Joins\n\nLeft anti joins are the opposite of left semi joins. Like left semi joins, they do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that do not have a corresponding key in the second DataFrame. Think of anti joins as a NOT IN SQL-style filter:"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-------+----------+-----------+\n| id| degree|department|     school|\n+---+-------+----------+-----------+\n|  2|Masters|      EECS|UC Berkeley|\n+---+-------+----------+-----------+\n\n"}], "source": "graduateProgram.join(person, \n                     on = person[\"graduate_program\"] == graduateProgram['id'],\n                     how = \"left_anti\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT * FROM graduateProgram \nLEFT ANTI JOIN person\nON graduateProgram.id = person.graduate_program\n```\n\n## Cross (Cartesian) Joins\n\nThe last of our joins are cross-joins or cartesian products. Cross-joins in simplest terms are inner joins that do not specify a predicate. Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. If you have 1,000 rows in each DataFrame, the cross-join of these will result in 1,000,000 (1,000 x 1,000) rows. For this reason, you must very explicitly state that you want a cross-join by using the cross join keyword:"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n|  0|   Bill Chambers|               0|          [100]|  2|Masters|                EECS|UC Berkeley|\n|  0|   Bill Chambers|               0|          [100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  2|Masters|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  0|Masters|School of Informa...|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  2|Masters|                EECS|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n"}], "source": "person.crossJoin(graduateProgram).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "# Challenges When Using Joins\n\nWhen performing joins, there are some specific challenges and some common questions that arise.\n\n## Handling Duplicate Column Names\n\nOne of the tricky things that come up in joins is dealing with duplicate column names in your results DataFrame. In a DataFrame, each column has a unique ID within Spark\u2019s SQL Engine, Catalyst. This unique ID is purely internal and not something that you can directly reference. This makes it quite difficult to refer to a specific column when you have a DataFrame with duplicate column names.\n\nThis can occur in two distinct situations:\n\n* The join expression that you specify does not remove one key from one of the input DataFrames and the keys have the same column name\n\n* Two columns on which you are not performing the join have the same name\n\nLet\u2019s create a problem dataset that we can use to illustrate these problems:"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n\n"}], "source": "gradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")\n\ndf = person.join(gradProgramDupe,\n                 on = person[\"graduate_program\"] == gradProgramDupe[\"graduate_program\"],\n                 how = \"inner\")\ndf.show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Note that there are now two graduate_program columns, even though we joined on that key:\n\nNow if we run the following command we will get an error because of the ambiguous column name:\n\n```python\ndf.select(\"graduate_program\").show()\n```\n\nError:\n```\nAnalysisException: \"Reference 'graduate_program' is ambiguous, could be: graduate_program, graduate_program.;\n```\n\n### APPROACH 1: DIFFERENT JOIN EXPRESSION\n\nWhen you have two keys that have the same name, probably the easiest fix is to change the join expression from a Boolean expression to a string or sequence. This automatically removes one of the columns for you during the join:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+---+----------------+---------------+-------+--------------------+-----------+\n|graduate_program| id|            name|   spark_status| degree|          department|     school|\n+----------------+---+----------------+---------------+-------+--------------------+-----------+\n|               0|  0|   Bill Chambers|          [100]|Masters|School of Informa...|UC Berkeley|\n|               1|  2|Michael Armbrust|     [250, 100]|  Ph.D.|                EECS|UC Berkeley|\n|               1|  1|   Matei Zaharia|[500, 250, 100]|  Ph.D.|                EECS|UC Berkeley|\n+----------------+---+----------------+---------------+-------+--------------------+-----------+\n\n"}], "source": "person.join(gradProgramDupe,\"graduate_program\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### APPROACH 2: DROPPING THE COLUMN AFTER THE JOIN\n\nAnother approach is to drop the offending column after the join. When doing this, we need to refer to the column via the original source DataFrame. We can do this if the join uses the same key names or if the source DataFrames have columns that simply have the same name:"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------------+---------------+----------------+-------+--------------------+-----------+\n| id|            name|   spark_status|graduate_program| degree|          department|     school|\n+---+----------------+---------------+----------------+-------+--------------------+-----------+\n|  0|   Bill Chambers|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n|  2|Michael Armbrust|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+---------------+----------------+-------+--------------------+-----------+\n\n"}], "source": "person.join(gradProgramDupe, \n           person[\"graduate_program\"] == gradProgramDupe[\"graduate_program\"])\\\n  .drop(person[\"graduate_program\"]).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### APPROACH 3: RENAMING A COLUMN BEFORE THE JOIN\n\nWe can avoid this issue altogether if we rename one of our columns before the join:"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n| id|            name|graduate_program|   spark_status|grad_id| degree|          department|     school|\n+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n|  0|   Bill Chambers|               0|          [100]|      0|Masters|School of Informa...|UC Berkeley|\n|  2|Michael Armbrust|               1|     [250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n|  1|   Matei Zaharia|               1|[500, 250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n\n"}], "source": "gradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\n\nperson.join(gradProgram3, \n           person[\"graduate_program\"] == gradProgram3[\"grad_id\"]).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "# How Spark Performs Joins\n\nTo understand how Spark performs joins, you need to understand the two core resources at play: the node-to-node communication strategy and per node computation strategy. These internals are likely irrelevant to your business problem. However, comprehending how Spark performs joins can mean the difference between a job that completes quickly and one that never completes at all.\n\n## Communication Strategies\n\nSpark approaches cluster communication in three different ways during joins. It either incurs a **sort-merge join**, which results in an all-to-all communication, a **broadcast join**, or **shuffle-hash join**. The core foundation of our simplified view of joins is that in Spark you will have either a big table or a small table. Although this is obviously a spectrum (and things do happen differently if you have a \u201cmedium-sized table\u201d), it can help to be binary about the distinction for the sake of this explanation.\n\n### BIG TABLE\u2013TO\u2013BIG TABLE\n\nWhen you join a big table to another big table, you end up with a sort-merge join. This behavior is standard as Spark 3.0 and as of the writing of this notebook. Prior to this version of Spark shuffle hash join was the default, but it has changed since then. The implementation of sort-merge Join in Spark is similar to any other SQL engine, except the fact that it happens over partitions because of the distributed nature of data. \n\nThe illustration below represents how the data moves around different executors to make the join possible. If the DataFrames sharing the same partitioner are materialized by the same action, they will end up being co-located.\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/06-03-Joining-two-big-tables.png?raw=true\" width=\"700\" align=\"center\"/>"}, {"cell_type": "markdown", "metadata": {}, "source": "In a sort-merge join, every node talks to every other node and they share data according to which node has a certain key or set of keys (on which you are joining). These joins are expensive because the network can become congested with traffic, especially if your DataFrames are not co-partitioned.\n\nThis join describes taking a big table of data and joining it to another big table of data. An example of this might be a company that receives billions of messages every day from the Internet of Things, and needs to identify the day-over-day changes that have occurred. The way to do this is by joining on *deviceId*, *messageType*, and *date* in one column, and *date - 1 day* in the other column.\n\nIn the figure above, DataFrame 1 and DataFrame 2 are both large DataFrames. This means that all worker nodes (and potentially every partition) will need to communicate with one another during the entire join process (with no intelligent partitioning of data).\n\n### BIG TABLE\u2013TO\u2013SMALL TABLE\n\nWhen the table is small enough to fit into the memory of a single worker node, with some breathing room of course, we can optimize our join. Although we can use a big table\u2013to\u2013big table communication strategy, it can often be more efficient to use a broadcast join. What this means is that we will replicate our small DataFrame onto every worker node in the cluster (be it located on one machine or many). Now this sounds expensive. However, what this does is to prevent us from performing the all-to-all communication during the entire join process. Instead, we perform it only once at the beginning and then let each individual worker node perform the work without having to wait or communicate with any other worker node, as is depicted in the figure below:\n\n<img src=\"https://github.com/soltaniehha/Big-Data-Analytics-for-Business/blob/master/figs/06-03-A-broadcast-join.png?raw=true\" width=\"700\" align=\"center\"/>"}, {"cell_type": "markdown", "metadata": {}, "source": "At the beginning of this join will be a large communication, just like in the previous type of join. However, immediately after that first, there will be no further communication between nodes. This means that joins will be performed on every single node individually, making CPU the biggest bottleneck. For our current set of data, we can see that Spark has automatically set this up as a sort-merge join by looking at the explain plan:"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- SortMergeJoin [graduate_program#10L], [id#24L], Inner\n   :- Sort [graduate_program#10L ASC NULLS FIRST], false, 0\n   :  +- Exchange hashpartitioning(graduate_program#10L, 200), ENSURE_REQUIREMENTS, [id=#1552]\n   :     +- Project [_1#0L AS id#8L, _2#1 AS name#9, _3#2L AS graduate_program#10L, _4#3 AS spark_status#11]\n   :        +- Filter isnotnull(_3#2L)\n   :           +- Scan ExistingRDD[_1#0L,_2#1,_3#2L,_4#3]\n   +- Sort [id#24L ASC NULLS FIRST], false, 0\n      +- Exchange hashpartitioning(id#24L, 200), ENSURE_REQUIREMENTS, [id=#1553]\n         +- Project [_1#16L AS id#24L, _2#17 AS degree#25, _3#18 AS department#26, _4#19 AS school#27]\n            +- Filter isnotnull(_1#16L)\n               +- Scan ExistingRDD[_1#16L,_2#17,_3#18,_4#19]\n\n\n"}], "source": "person.join(graduateProgram, \n           person[\"graduate_program\"] == graduateProgram[\"id\"]).explain()"}, {"cell_type": "markdown", "metadata": {}, "source": "With the DataFrame API, we can explicitly give the optimizer a hint that we would like to use a broadcast join by using the correct function around the small DataFrame in question:"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- BroadcastHashJoin [graduate_program#10L], [id#24L], Inner, BuildRight, false\n   :- Project [_1#0L AS id#8L, _2#1 AS name#9, _3#2L AS graduate_program#10L, _4#3 AS spark_status#11]\n   :  +- Filter isnotnull(_3#2L)\n   :     +- Scan ExistingRDD[_1#0L,_2#1,_3#2L,_4#3]\n   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [id=#1585]\n      +- Project [_1#16L AS id#24L, _2#17 AS degree#25, _3#18 AS department#26, _4#19 AS school#27]\n         +- Filter isnotnull(_1#16L)\n            +- Scan ExistingRDD[_1#16L,_2#17,_3#18,_4#19]\n\n\n"}], "source": "from pyspark.sql.functions import broadcast\n\nperson.join(broadcast(graduateProgram), \n           person[\"graduate_program\"] == graduateProgram[\"id\"]).explain()"}, {"cell_type": "markdown", "metadata": {}, "source": "if you try to broadcast something too large, you can crash your driver node (because that collect is expensive). This is likely an area for optimization in the future.\n\n### LITTLE TABLE\u2013TO\u2013LITTLE TABLE\n\nWhen performing joins with small tables, it\u2019s usually best to let Spark decide how to join them. You can always force a broadcast join if you\u2019re noticing strange behavior."}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}}, "nbformat": 4, "nbformat_minor": 4}