{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Aggregations\n\nSpark allows us to create several different grouping types for aggregation. This notebook will discuss some of these grouping and aggregation techniques.\n\nLet's first load some retail data. We have over 300 csv files each representing daily transactions in a retail store:"}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": "data = \"gs://is843-demo/notebooks/jupyter/data/\""}, {"cell_type": "markdown", "metadata": {}, "source": "## Data import"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(data + \"retail-data/by-day/*.csv\")"}, {"cell_type": "markdown", "metadata": {}, "source": "Here\u2019s a sample of the data:"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\n|InvoiceNo|StockCode|Description                    |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\n|580538   |23084    |RABBIT NIGHT LIGHT             |48      |2011-12-05 08:38:00|1.79     |14075.0   |United Kingdom|\n|580538   |23077    |DOUGHNUT LIP GLOSS             |20      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|\n|580538   |22906    |12 MESSAGE CARDS WITH ENVELOPES|24      |2011-12-05 08:38:00|1.65     |14075.0   |United Kingdom|\n|580538   |21914    |BLUE HARMONICA IN BOX          |24      |2011-12-05 08:38:00|1.25     |14075.0   |United Kingdom|\n|580538   |22467    |GUMBALL COAT RACK              |6       |2011-12-05 08:38:00|2.55     |14075.0   |United Kingdom|\n+---------+---------+-------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 5 rows\n\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: integer (nullable = true)\n |-- InvoiceDate: timestamp (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: double (nullable = true)\n |-- Country: string (nullable = true)\n\n"}], "source": "df.show(5, False)\n\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "`InvoiceDate` is being recognized as string. Let's replace it with a date format. We won't need the timestamp, so it's ok to lose it.\n\nWe will also cast `CustomerID` as string and Quantity as long:"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n|   580538|    23084|  RABBIT NIGHT LIGHT|      48| 2011-12-05|     1.79|   14075.0|United Kingdom|\n|   580538|    23077| DOUGHNUT LIP GLOSS |      20| 2011-12-05|     1.25|   14075.0|United Kingdom|\n|   580538|    22906|12 MESSAGE CARDS ...|      24| 2011-12-05|     1.65|   14075.0|United Kingdom|\n|   580538|    21914|BLUE HARMONICA IN...|      24| 2011-12-05|     1.25|   14075.0|United Kingdom|\n|   580538|    22467|   GUMBALL COAT RACK|       6| 2011-12-05|     2.55|   14075.0|United Kingdom|\n+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\nonly showing top 5 rows\n\nroot\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: long (nullable = true)\n |-- InvoiceDate: date (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- Country: string (nullable = true)\n\n"}], "source": "from pyspark.sql.functions import col, to_date\n\ndf = df.withColumn(\"InvoiceDate\", to_date(col(\"InvoiceDate\"), \"yyyy-MM-dd HH:mm:ss\"))\n\ndf = df.withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"string\"))\ndf = df.withColumn(\"Quantity\", col(\"Quantity\").cast(\"long\"))\n\ndf.createOrReplaceTempView(\"dfTable\")\n\ndf.show(5)\ndf.printSchema()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Caching\n\nCaching allows the DataFrame to be loaded and persist in the memory. If we don't use this option, every time we execute an action our DataFrame gets loaded from our Cloud Storage, which is not ideal and will add to our execution time:\n\n**Note:** Caching is a lazy transformation. It will happen the first time you execute an action against the DataFrame, not when you cache that DataFrame."}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"data": {"text/plain": "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: bigint, InvoiceDate: date, UnitPrice: double, CustomerID: string, Country: string]"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "df.cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "Basic aggregations apply to an entire DataFrame. The simplest example is the `count` method:"}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/plain": "541909"}, "execution_count": 6, "metadata": {}, "output_type": "execute_result"}], "source": "df.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "It returns the total number of rows in this DataFrame. `count` when used in this context is actually an action, so it returns the output immediately. We will also encounter cases that `count` will be acting as a lazy transformation.\n\nNow that we have performed an action on `df` it should be cached into the memory. Go ahead an execute the previous command again to see the performance gain:"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"data": {"text/plain": "541909"}, "execution_count": 7, "metadata": {}, "output_type": "execute_result"}], "source": "df.count()"}, {"cell_type": "markdown", "metadata": {}, "source": "Once done with the DataFrame we can free up the memory by removing the cache. This can be done by `unpersist`:\n```python\ndf.unpersist()\n```"}, {"cell_type": "markdown", "metadata": {}, "source": "## Aggregation Functions\n\nAll aggregations are available as functions, in addition to the special cases that can appear on DataFrames or via `.stat`. You can find most aggregation functions in the `pyspark.sql.functions` package.\n\n**Note:** There are some gaps between the available SQL functions and the functions that we can import in Scala and Python. This changes every release, so it\u2019s impossible to include a definitive list. This section covers the most common functions.\n\n### `count`\nThe first function worth going over is `count`, except in this example it will perform as a transformation instead of an action. In this case, we can do one of two things: specify a specific column to count, or all the columns by using `count(*)` or `count(1)` to represent that we want to count every row as the literal one, as shown in this example:"}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+----------------+\n|count(StockCode)|\n+----------------+\n|          541909|\n+----------------+\n\n"}], "source": "from pyspark.sql.functions import count\n\ndf.select(count(\"StockCode\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT COUNT(*) FROM dfTable\n```\n\n**Warning**\n\nwhen performing a `count(*)`, Spark will count null values (including rows containing all nulls). However, when counting an individual column, Spark will not count the null values. For instance if we repeate this for \"CustomerID\" column we will get a different value due to the null values:"}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+\n|count(CustomerID)|\n+-----------------+\n|           406829|\n+-----------------+\n\n"}], "source": "df.select(count(\"CustomerID\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### `countDistinct`\n\nSometimes, the total number is not relevant; rather, it\u2019s the number of unique groups that you want. To get this number, you can use the `countDistinct` function. This is a bit more relevant for individual columns:"}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 18:=============================================>           (8 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4070|\n+-------------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import countDistinct\n\ndf.select(countDistinct(\"StockCode\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT COUNT(DISTINCT *) FROM DFTABLE\n```\n\n`approx_count_distinct`: This function can be used to estimate the count distinct. It will give us a lot of performace gain.\n\n### `min` and `max`\n\nTo extract the minimum and maximum values from a DataFrame, use the min and max functions:"}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+-------------+\n|min(Quantity)|max(Quantity)|\n+-------------+-------------+\n|       -80995|        80995|\n+-------------+-------------+\n\n"}], "source": "from pyspark.sql.functions import min, max\n\ndf.select(min(\"Quantity\"), max(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### `sum`\n\nAnother simple task is to add all the values in a row using the sum function:"}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------+\n|sum(Quantity)|\n+-------------+\n|      5176450|\n+-------------+\n\n"}], "source": "from pyspark.sql.functions import sum\n\ndf.select(sum(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### `avg`\n\n`avg` or `mean` functions:"}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+--------------------------------------+----------------+----------------+\n|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n|                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n"}], "source": "from pyspark.sql.functions import sum, count, avg, expr\n\ndf.select(\n    count(\"Quantity\").alias(\"total_transactions\"),\n    sum(\"Quantity\").alias(\"total_purchases\"),\n    avg(\"Quantity\").alias(\"avg_purchases\"),\n    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n  .selectExpr(\n    \"total_purchases/total_transactions\",\n    \"avg_purchases\",\n    \"mean_purchases\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Variance and Standard Deviation\n\nCalculating the mean naturally brings up questions about the variance and standard deviation. These are both measures of the spread of the data around the mean. The variance is the average of the squared differences from the mean, and the standard deviation is the square root of the variance. You can calculate these in Spark by using their respective functions:"}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+------------------+---------------------+\n|var_samp(Quantity)|stddev_samp(Quantity)|\n+------------------+---------------------+\n| 47559.39140929847|   218.08115785023352|\n+------------------+---------------------+\n\n"}], "source": "from pyspark.sql.functions import variance, stddev\n\ndf.select(variance(\"Quantity\"), stddev(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Something to note is that Spark has both the formula for the sample standard deviation as well as the formula for the population standard deviation. These are fundamentally different statistical formulae, and we need to differentiate between them. By default, Spark performs the formula for the sample standard deviation or variance if you use the `variance` or `stddev` functions.\n\nYou can also specify these explicitly or refer to the population standard deviation or variance:"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------------+------------------+--------------------+---------------------+\n|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n+-----------------+------------------+--------------------+---------------------+\n|47559.30364660878| 47559.39140929847|  218.08095663447733|   218.08115785023352|\n+-----------------+------------------+--------------------+---------------------+\n\n"}], "source": "from pyspark.sql.functions import var_pop, stddev_pop\nfrom pyspark.sql.functions import var_samp, stddev_samp\n\ndf.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT var_pop(Quantity), var_samp(Quantity),\n  stddev_pop(Quantity), stddev_samp(Quantity)\nFROM dfTable\n```\n\n### Covariance and Correlation\n\nWe discussed single column aggregations, but some functions compare the interactions of the values in two difference columns together. Two of these functions are `covar_samp` and `corr`, for covariance and correlation, respectively. Correlation measures the Pearson correlation coefficient, which is scaled between \u20131 and +1. The covariance is scaled according to the inputs in the data. There also exists a covariance function for the population, covar_pop."}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------------+-------------------------------+\n|corr(UnitPrice, Quantity)|covar_samp(UnitPrice, Quantity)|\n+-------------------------+-------------------------------+\n|     -0.00123492454487...|            -26.058761257936645|\n+-------------------------+-------------------------------+\n\n"}], "source": "from pyspark.sql.functions import corr, covar_samp\n\ndf.select(corr(\"UnitPrice\", \"Quantity\"), covar_samp(\"UnitPrice\", \"Quantity\")).show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Grouping\n\nThus far, we have performed only DataFrame-level aggregations. A more common task is to perform calculations based on groups in the data. This is typically done on categorical data for which we group our data on one column and perform some calculations on the other columns that end up in that group.\n\nThe best way to explain this is to begin performing some groupings. The first will be a count, just as we did before. We will group by each unique invoice number and get the count of items on that invoice. Note that this returns another DataFrame and is lazily performed.\n\nWe do this grouping in two phases. First we specify the column(s) on which we would like to group, and then we specify the aggregation(s). The first step returns a `RelationalGroupedDataset`, and the second step returns a `DataFrame`.\n\nAs mentioned, we can specify any number of columns on which we want to group:"}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [{"data": {"text/plain": "<pyspark.sql.group.GroupedData at 0x7fd1bcf26c20>"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "df.groupBy(\"InvoiceNo\", \"CustomerId\")"}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 42:===================================================>     (9 + 1) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----------+-----+\n|InvoiceNo|CustomerId|count|\n+---------+----------+-----+\n|   577728|   17811.0|   46|\n|   580060|   14287.0|    5|\n|   580066|   14309.0|   18|\n|   577170|   13456.0|    7|\n|   580193|   16899.0|   25|\n+---------+----------+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "In SQL\n```sql\nSELECT InvoiceNo, CustomerId, count(*) FROM dfTable GROUP BY InvoiceNo, CustomerId\n```\n\n## Grouping with Expressions\n\nAs we saw earlier, counting is a bit of a special case because it exists as a method. For this, usually we prefer to use the `count` function. Rather than passing that function as an expression into a `select` statement, we specify it as within `agg`. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like `alias` a column after transforming it for later use in your data flow:"}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 45:=============================================>           (8 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+----+-----+\n|InvoiceNo|quan|quan2|\n+---------+----+-----+\n|   573409|   1|    1|\n|   572458|  26|   26|\n|  C577362|   1|    1|\n|   568711|   4|    4|\n|   538041|   1|    1|\n+---------+----+-----+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import count, expr\n\ndf.groupBy(\"InvoiceNo\")\\\n  .agg(count(\"Quantity\").alias(\"quan\"),\n       expr(\"count(Quantity) as quan2\")\n      ).show(5)"}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 48:==================================>                      (6 + 4) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---------+------------------+--------------------+\n|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n+---------+------------------+--------------------+\n|   574966|               6.0|   3.640054944640259|\n|   575091|11.552631578947368|   5.008925551458656|\n|   578057| 4.607142857142857|   8.755974636597271|\n|   537252|              31.0|                 0.0|\n|   578459|              28.0|                26.0|\n+---------+------------------+--------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Window Functions\n\nWindow functions operate on a set of rows and return a single value for each row from the underlying query. The term window describes the set of rows on which the function operates. A window function uses values from the rows in a window to calculate the returned values.\n\nThe following SQL query adds a new colummn (`overall_dataset_Q`) that includes the overall quantity for the entire dataset. This value will be the same for all the rows:"}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/21 21:22:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 53:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|\n+----------+-----------+--------+-----------------+\n|   12346.0| 2011-01-18|  -74215|          4906888|\n|   12346.0| 2011-01-18|   74215|          4906888|\n|   12347.0| 2011-12-07|      12|          4906888|\n|   12347.0| 2011-12-07|      24|          4906888|\n|   12347.0| 2011-12-07|      24|          4906888|\n+----------+-----------+--------+-----------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC\n\"\"\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "`OVER()` clause indicates that we want to use a window over the `sum()` expression. \n\nThe `OVER()` clause has the following capabilities:\n\n* Defines window partitions to form groups of rows. (`PARTITION BY` clause)\n* Orders rows within a partition. (`ORDER BY` clause)\n\nIn the above query since we haven't defined a `PARTITION` within `OVER()` the aggregating function applies to the entire dataset.\n\nWe can do a lot more useful calculations with the window functions. For instance, we can add a total quantity per customer and a total quantity per customer by date, specifying the appropriate `PARTITION`s:"}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/21 21:22:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:41 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 56:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+---------------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|total_Q_by_date|\n+----------+-----------+--------+-----------------+-------+---------------+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|              0|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|              0|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|\n|   12347.0| 2011-10-31|       4|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|       8|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      12|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      12|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      20|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      12|          4906888|   2458|            676|\n|   12347.0| 2011-10-31|      12|          4906888|   2458|            676|\n+----------+-----------+--------+-----------------+-------+---------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Using `ORDER BY` clause we can sort the rows within the `PARTITION`s and then use the `rank()` function to give them a ranking:"}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/21 21:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 59:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+---------------+----+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|total_Q_by_date|rank|\n+----------+-----------+--------+-----------------+-------+---------------+----+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|              0|   1|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|              0|   2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|            192|   1|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|   5|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|            192|   5|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|            192|   7|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|   8|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|            192|   8|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|            192|  10|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|            192|  11|\n|   12347.0| 2011-10-31|      48|          4906888|   2458|            676|   1|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|            676|   2|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|            676|   2|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|            676|   4|\n+----------+-----------+--------+-----------------+-------+---------------+----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date,\n  \n  RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n               ORDER BY Quantity DESC) as rank\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC, rank\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "Please notice the behavior of `rank()` when it comes across tied values.\n\n`dense_rank()` returns the rank of rows within a window partition, without any gaps.\n\n`row_number()` returns a sequential number starting at 1 within a window partition. Please note that the ties will be numbered at random. This could have downstream consequences!\n\nIn the example below we have added both `dense_rank()` and `row_number()` to the previous query:"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/21 21:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:45 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:46 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 62:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|max_Q|total_Q_by_date|rank|d_rank|row_number|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|74215|              0|   1|     1|         1|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|74215|              0|   2|     2|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         3|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         4|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         5|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         6|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|  240|            192|   7|     3|         7|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         8|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         9|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|  240|            192|  10|     5|        10|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|  240|            192|  11|     6|        11|\n|   12347.0| 2011-10-31|      48|          4906888|   2458|  240|            676|   1|     1|         1|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         2|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         3|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         5|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         6|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         7|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "spark.sql(\"\"\"\nSELECT CustomerId, InvoiceDate, Quantity,\n\n  sum(Quantity) OVER () as overall_dataset_Q,\n  \n  sum(Quantity) OVER (PARTITION BY CustomerId) as total_Q,\n  \n  max(Quantity) OVER (PARTITION BY CustomerId) as max_Q,\n\n  sum(Quantity) OVER (PARTITION BY CustomerId, InvoiceDate) as total_Q_by_date,\n    \n  RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n               ORDER BY Quantity DESC) as rank,\n  \n  DENSE_RANK() OVER (PARTITION BY CustomerId, InvoiceDate \n                     ORDER BY Quantity DESC) as d_rank,\n  \n  ROW_NUMBER() OVER (PARTITION BY CustomerId, InvoiceDate \n                     ORDER BY Quantity DESC) as row_number\n  \nFROM dfTable WHERE CustomerId IS NOT NULL \nORDER BY CustomerId, InvoiceDate DESC, row_number\n\"\"\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "A few points regarding ranking functions:\n\n* `ROW_NUMBER()`, `RANK()`, and other ranking functions must always be windowed and therefore cannot appear without a corresponding OVER clause.\n\n* Give consideration to how ties should be handled with ranking functions. If you need contiguous ranking, you should use `DENSE_RANK()` instead.\n\n* The `ORDER BY` predicate is mandatory for this class of functions because it influences how the results will be sequenced or ranked."}, {"cell_type": "markdown", "metadata": {}, "source": "### PySpark Syntax\n\nNow that we are familiar with the SQL syntax of window functions let's have a look at its PySpark equivalent.\n\nThe first step to a window function is to create a window specification. \n\nNote that the `partition by` is unrelated to the partitioning scheme concept that we have covered thus far. It\u2019s just a similar concept that describes how we will be breaking up our group. The ordering determines the ordering within a given partition. \n\nIn the following example, we will reproduce our last SQL query:"}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [], "source": "from pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\n\nwindowSpec0 = Window.partitionBy()\nwindowSpec1 = Window.partitionBy(\"CustomerId\")\nwindowSpec2 = Window.partitionBy(\"CustomerId\", \"InvoiceDate\")\nwindowSpec3 = Window.partitionBy(\"CustomerId\", \"InvoiceDate\").orderBy(desc(\"Quantity\"))"}, {"cell_type": "markdown", "metadata": {}, "source": "Now we want to use an aggregation function to learn more about each specific customer. An example might be establishing the maximum purchase quantity on each day. We indicate the window specification that defines to which frames of data this function will apply:"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import sum, max, rank, dense_rank, row_number\n\noverall_dataset_Q = sum(col(\"Quantity\")).over(windowSpec0)\ntotal_Q = sum(col(\"Quantity\")).over(windowSpec1)\nmax_Q = max(col(\"Quantity\")).over(windowSpec1)\ntotal_Q_by_date = sum(col(\"Quantity\")).over(windowSpec2)\nrank = rank().over(windowSpec3)\nd_rank = dense_rank().over(windowSpec3)\nrow_number = row_number().over(windowSpec3)"}, {"cell_type": "markdown", "metadata": {}, "source": "This returns columns that we can use in `select` statements. Now we can perform a select to view the calculated window values:"}, {"cell_type": "code", "execution_count": 27, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/21 21:22:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:47 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n23/03/21 21:22:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n[Stage 65:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|CustomerId|InvoiceDate|Quantity|overall_dataset_Q|total_Q|max_Q|total_Q_by_date|rank|d_rank|row_number|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\n|   12346.0| 2011-01-18|   74215|          4906888|      0|74215|              0|   1|     1|         1|\n|   12346.0| 2011-01-18|  -74215|          4906888|      0|74215|              0|   2|     2|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         1|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         2|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         3|\n|   12347.0| 2011-12-07|      24|          4906888|   2458|  240|            192|   1|     1|         4|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         5|\n|   12347.0| 2011-12-07|      20|          4906888|   2458|  240|            192|   5|     2|         6|\n|   12347.0| 2011-12-07|      16|          4906888|   2458|  240|            192|   7|     3|         7|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         8|\n|   12347.0| 2011-12-07|      12|          4906888|   2458|  240|            192|   8|     4|         9|\n|   12347.0| 2011-12-07|      10|          4906888|   2458|  240|            192|  10|     5|        10|\n|   12347.0| 2011-12-07|       6|          4906888|   2458|  240|            192|  11|     6|        11|\n|   12347.0| 2011-10-31|      48|          4906888|   2458|  240|            676|   1|     1|         1|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         2|\n|   12347.0| 2011-10-31|      36|          4906888|   2458|  240|            676|   2|     2|         3|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         4|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         5|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         6|\n|   12347.0| 2011-10-31|      24|          4906888|   2458|  240|            676|   4|     3|         7|\n+----------+-----------+--------+-----------------+-------+-----+---------------+----+------+----------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col\n\ndf.where(\"CustomerId IS NOT NULL\")\\\n  .select(\n    col(\"CustomerId\"),\n    col(\"InvoiceDate\"),\n    col(\"Quantity\"),\n    overall_dataset_Q.alias(\"overall_dataset_Q\"),\n    total_Q.alias(\"total_Q\"),\n    max_Q.alias(\"max_Q\"),\n    total_Q_by_date.alias(\"total_Q_by_date\"),\n    rank.alias(\"rank\"),\n    d_rank.alias(\"d_rank\"),\n    row_number.alias(\"row_number\"))\\\n  .orderBy(\"CustomerId\", desc(\"InvoiceDate\"), \"row_number\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Rollups\n\nA rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us.\n\nLet\u2019s create a rollup that looks across time (with our new `InvoiceDate` column) and space (with the `Country` column) and creates a new DataFrame that includes \n- the grand total over all dates \n- the grand total for each date in the DataFrame \n- and the subtotal for each country on each date in the DataFrame"}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": "dfNoNull = df.na.drop()\ndfNoNull.createOrReplaceTempView(\"dfNoNull\")"}, {"cell_type": "code", "execution_count": 29, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 66:=============================================>           (8 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------------+-------------+\n|InvoiceDate|       Country|sum(Quantity)|\n+-----------+--------------+-------------+\n|       null|          null|      4906888|\n| 2010-12-01|        France|          449|\n| 2010-12-01|          EIRE|          243|\n| 2010-12-01|       Germany|          117|\n| 2010-12-01|        Norway|         1852|\n| 2010-12-01|     Australia|          107|\n| 2010-12-01|   Netherlands|           97|\n| 2010-12-01|          null|        24032|\n| 2010-12-01|United Kingdom|        21167|\n| 2010-12-02|          null|        20855|\n+-----------+--------------+-------------+\nonly showing top 10 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "rolledUpDF = dfNoNull.rollup(\"InvoiceDate\", \"Country\")\\\n  .agg(sum(\"Quantity\"))\\\n  .orderBy(\"InvoiceDate\")\n\nrolledUpDF.show(10)"}, {"cell_type": "markdown", "metadata": {}, "source": "Now where you see the null values is where you\u2019ll find the grand totals:"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------------+\n|InvoiceDate|Country|sum(Quantity)|\n+-----------+-------+-------------+\n| 2010-12-01|   null|        24032|\n| 2010-12-02|   null|        20855|\n| 2010-12-03|   null|        11548|\n| 2010-12-05|   null|        16394|\n| 2010-12-06|   null|        16095|\n+-----------+-------+-------------+\nonly showing top 5 rows\n\n"}], "source": "rolledUpDF.where(\"Country IS NULL\").where(\"InvoiceDate IS NOT NULL\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "A null in both rollup columns specifies the grand total across both of those columns:"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------------+\n|InvoiceDate|Country|sum(Quantity)|\n+-----------+-------+-------------+\n|       null|   null|      4906888|\n+-----------+-------+-------------+\n\n"}], "source": "rolledUpDF.where(\"InvoiceDate IS NULL\").show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Cube\n\nA `cube` takes the `rollup` to a level deeper. Rather than treating elements hierarchically, a cube does the same thing across all dimensions. This means that it won\u2019t just go by date over the entire time period, but also the country. To pose this as a question again, can you make a table that includes the following?\n\n* The total across all dates and countries\n* The total for each date across all countries\n* The total for each country on each date\n* The total for each country across all dates\n\nThe method call is quite similar, but instead of calling `rollup`, we call `cube`:"}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 75:==================================>                      (6 + 4) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------------------+-------------+\n|InvoiceDate|             Country|sum(Quantity)|\n+-----------+--------------------+-------------+\n|       null|United Arab Emirates|          982|\n|       null|               Japan|        25218|\n|       null|             Germany|       117448|\n|       null|           Lithuania|          652|\n|       null|                 USA|         1034|\n+-----------+--------------------+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import sum\n\ncubbedDf = dfNoNull.cube(\"InvoiceDate\", \"Country\")\\\n  .agg(sum(\"Quantity\"))\\\n  .orderBy(\"InvoiceDate\")\n\ncubbedDf.show(5)"}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-------+-------------+\n|InvoiceDate|Country|sum(Quantity)|\n+-----------+-------+-------------+\n|       null|   null|      4906888|\n| 2010-12-01|   null|        24032|\n| 2010-12-02|   null|        20855|\n| 2010-12-03|   null|        11548|\n| 2010-12-05|   null|        16394|\n+-----------+-------+-------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "cubbedDf.where(\"Country IS NULL\").show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "It\u2019s a great way to create a quick summary table that others can use later on."}, {"cell_type": "markdown", "metadata": {}, "source": "## Pivot\n\nPivots make it possible for you to convert a row into a column. For example, in our current data we have a `Country` column. With a `pivot`, we can aggregate according to some function for each of those given countries and display them in an easy-to-query way:"}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- InvoiceNo: string (nullable = true)\n |-- StockCode: string (nullable = true)\n |-- Description: string (nullable = true)\n |-- Quantity: long (nullable = true)\n |-- InvoiceDate: date (nullable = true)\n |-- UnitPrice: double (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- Country: string (nullable = true)\n\n"}], "source": "df.printSchema()"}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": "pivoted = df.groupBy(\"InvoiceDate\").pivot(\"Country\").sum()"}, {"cell_type": "markdown", "metadata": {}, "source": "This DataFrame will now have a column for every combination of country, numeric variable, and a column specifying the date. For example, for USA we have the following columns: USA_sum(Quantity), and USA_sum(UnitPrice). This represents one for each numeric column in our dataset (because we just performed an aggregation over all of them).\n\nHere\u2019s an example query and result from this data:"}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/03/21 21:22:56 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n[Stage 89:=======================================>                 (7 + 2) / 10]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n|InvoiceDate|Australia_sum(Quantity)|Australia_sum(UnitPrice)|Austria_sum(Quantity)|Austria_sum(UnitPrice)|Bahrain_sum(Quantity)|Bahrain_sum(UnitPrice)|Belgium_sum(Quantity)|Belgium_sum(UnitPrice)|Brazil_sum(Quantity)|Brazil_sum(UnitPrice)|Canada_sum(Quantity)|Canada_sum(UnitPrice)|Channel Islands_sum(Quantity)|Channel Islands_sum(UnitPrice)|Cyprus_sum(Quantity)|Cyprus_sum(UnitPrice)|Czech Republic_sum(Quantity)|Czech Republic_sum(UnitPrice)|Denmark_sum(Quantity)|Denmark_sum(UnitPrice)|EIRE_sum(Quantity)|EIRE_sum(UnitPrice)|European Community_sum(Quantity)|European Community_sum(UnitPrice)|Finland_sum(Quantity)|Finland_sum(UnitPrice)|France_sum(Quantity)|France_sum(UnitPrice)|Germany_sum(Quantity)|Germany_sum(UnitPrice)|Greece_sum(Quantity)|Greece_sum(UnitPrice)|Hong Kong_sum(Quantity)|Hong Kong_sum(UnitPrice)|Iceland_sum(Quantity)|Iceland_sum(UnitPrice)|Israel_sum(Quantity)|Israel_sum(UnitPrice)|Italy_sum(Quantity)|Italy_sum(UnitPrice)|Japan_sum(Quantity)|Japan_sum(UnitPrice)|Lebanon_sum(Quantity)|Lebanon_sum(UnitPrice)|Lithuania_sum(Quantity)|Lithuania_sum(UnitPrice)|Malta_sum(Quantity)|Malta_sum(UnitPrice)|Netherlands_sum(Quantity)|Netherlands_sum(UnitPrice)|Norway_sum(Quantity)|Norway_sum(UnitPrice)|Poland_sum(Quantity)|Poland_sum(UnitPrice)|Portugal_sum(Quantity)|Portugal_sum(UnitPrice)|RSA_sum(Quantity)|RSA_sum(UnitPrice)|Saudi Arabia_sum(Quantity)|Saudi Arabia_sum(UnitPrice)|Singapore_sum(Quantity)|Singapore_sum(UnitPrice)|Spain_sum(Quantity)|Spain_sum(UnitPrice)|Sweden_sum(Quantity)|Sweden_sum(UnitPrice)|Switzerland_sum(Quantity)|Switzerland_sum(UnitPrice)|USA_sum(Quantity)|USA_sum(UnitPrice)|United Arab Emirates_sum(Quantity)|United Arab Emirates_sum(UnitPrice)|United Kingdom_sum(Quantity)|United Kingdom_sum(UnitPrice)|Unspecified_sum(Quantity)|Unspecified_sum(UnitPrice)|\n+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n| 2011-10-07|                   null|                    null|                 null|                  null|                 null|                  null|                 null|                  null|                null|                 null|                null|                 null|                         null|                          null|                 345|   136.51000000000002|                         325|                        49.38|                  637|                 27.69|               448| 225.09999999999997|                            null|                             null|                 null|                  null|                 527|    88.43999999999998|                 2053|    327.79999999999995|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                      212|                     25.35|                null|                 null|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|                227|  123.08000000000001|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                       25657|           12425.409999999976|                     null|                      null|\n| 2011-01-30|                   null|                    null|                 null|                  null|                 null|                  null|                 null|                  null|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              null|               null|                            null|                             null|                 null|                  null|                null|                 null|                 null|                  null|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                     null|                      null|                null|                 null|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|               null|                null|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                        3367|            2321.720000000002|                     null|                      null|\n| 2011-05-06|                   null|                    null|                   42|                 58.95|                 null|                  null|                  182|                 63.32|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              1694|              75.37|                            null|                             null|                 null|                  null|                null|                 null|                  222|    12.639999999999999|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                     null|                      null|                null|                 null|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|                 88|               113.1|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                       17404|            6952.629999999985|                     null|                      null|\n| 2011-11-18|                   null|                    null|                 null|                  null|                 null|                  null|                  315|     87.74000000000001|                null|                 null|                null|                 null|                         null|                          null|                 300|   245.99000000000004|                         -40|           6.9399999999999995|                 null|                  null|               263| 230.84999999999982|                            null|                             null|                 null|                  null|                 550|               167.77|                  491|    128.18000000000004|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|                544|  232.31999999999996|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                       34|                     19.25|                 242|   186.04000000000002|                  -8|    8.190000000000001|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|               null|                null|                  -3|                 2.89|                      669|         78.28999999999998|             null|              null|                              null|                               null|                       20178|           10676.199999999879|                     null|                      null|\n| 2011-07-07|                   null|                    null|                 null|                  null|                 null|                  null|                  143|     96.71000000000004|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              null|               null|                            null|                             null|                 null|                  null|                null|                 null|                  550|    175.46000000000006|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                     null|                      null|                null|                 null|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|                216|  139.00000000000003|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                       18047|            6223.269999999994|                     null|                      null|\n+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "pivoted.show(5)"}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n|InvoiceDate|Australia_sum(Quantity)|Australia_sum(UnitPrice)|Austria_sum(Quantity)|Austria_sum(UnitPrice)|Bahrain_sum(Quantity)|Bahrain_sum(UnitPrice)|Belgium_sum(Quantity)|Belgium_sum(UnitPrice)|Brazil_sum(Quantity)|Brazil_sum(UnitPrice)|Canada_sum(Quantity)|Canada_sum(UnitPrice)|Channel Islands_sum(Quantity)|Channel Islands_sum(UnitPrice)|Cyprus_sum(Quantity)|Cyprus_sum(UnitPrice)|Czech Republic_sum(Quantity)|Czech Republic_sum(UnitPrice)|Denmark_sum(Quantity)|Denmark_sum(UnitPrice)|EIRE_sum(Quantity)|EIRE_sum(UnitPrice)|European Community_sum(Quantity)|European Community_sum(UnitPrice)|Finland_sum(Quantity)|Finland_sum(UnitPrice)|France_sum(Quantity)|France_sum(UnitPrice)|Germany_sum(Quantity)|Germany_sum(UnitPrice)|Greece_sum(Quantity)|Greece_sum(UnitPrice)|Hong Kong_sum(Quantity)|Hong Kong_sum(UnitPrice)|Iceland_sum(Quantity)|Iceland_sum(UnitPrice)|Israel_sum(Quantity)|Israel_sum(UnitPrice)|Italy_sum(Quantity)|Italy_sum(UnitPrice)|Japan_sum(Quantity)|Japan_sum(UnitPrice)|Lebanon_sum(Quantity)|Lebanon_sum(UnitPrice)|Lithuania_sum(Quantity)|Lithuania_sum(UnitPrice)|Malta_sum(Quantity)|Malta_sum(UnitPrice)|Netherlands_sum(Quantity)|Netherlands_sum(UnitPrice)|Norway_sum(Quantity)|Norway_sum(UnitPrice)|Poland_sum(Quantity)|Poland_sum(UnitPrice)|Portugal_sum(Quantity)|Portugal_sum(UnitPrice)|RSA_sum(Quantity)|RSA_sum(UnitPrice)|Saudi Arabia_sum(Quantity)|Saudi Arabia_sum(UnitPrice)|Singapore_sum(Quantity)|Singapore_sum(UnitPrice)|Spain_sum(Quantity)|Spain_sum(UnitPrice)|Sweden_sum(Quantity)|Sweden_sum(UnitPrice)|Switzerland_sum(Quantity)|Switzerland_sum(UnitPrice)|USA_sum(Quantity)|USA_sum(UnitPrice)|United Arab Emirates_sum(Quantity)|United Arab Emirates_sum(UnitPrice)|United Kingdom_sum(Quantity)|United Kingdom_sum(UnitPrice)|Unspecified_sum(Quantity)|Unspecified_sum(UnitPrice)|\n+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n| 2011-12-06|                   null|                    null|                 null|                  null|                 null|                  null|                  396|    199.26000000000005|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              null|               null|                            null|                             null|                 null|                  null|                 787|    268.4799999999998|                  -29|                  4.15|                 236|   163.36000000000004|                   null|                    null|                 null|                  null|                null|                 null|                 45|               64.45|                -49|  19.679999999999996|                 null|                  null|                   null|                    null|               null|                null|                     null|                      null|                null|                 null|                null|                 null|                   267|      92.53000000000002|             null|              null|                      null|                       null|                   null|                    null|                182|               40.08|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                       27191|            9512.649999999947|                     null|                      null|\n| 2011-12-09|                   null|                    null|                 null|                  null|                 null|                  null|                  203|                 32.22|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              null|               null|                            null|                             null|                 null|                  null|                 105|                 44.5|                  880|    279.05999999999995|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                     null|                      null|                2227|   157.00000000000003|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|               null|                null|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                        9534|            8713.700000000063|                     null|                      null|\n| 2011-12-08|                   null|                    null|                  148|                 64.58|                 null|                  null|                 null|                  null|                null|                 null|                null|                 null|                           -1|                          4.25|                null|                 null|                        null|                         null|                 null|                  null|               806| 218.65999999999994|                            null|                             null|                 null|                  null|                  18|                 52.9|                  969|                192.33|                null|                 null|                   null|                    null|                 null|                  null|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                      140|                      1.79|                null|                 null|                null|                 null|                  null|                   null|             null|              null|                      null|                       null|                   null|                    null|               null|                null|                null|                 null|                     null|                      null|             -196|             13.75|                              null|                               null|                       32576|           21259.840000000513|                     null|                      null|\n| 2011-12-07|                   null|                    null|                 null|                  null|                 null|                  null|                 null|                  null|                null|                 null|                null|                 null|                         null|                          null|                null|                 null|                        null|                         null|                 null|                  null|              1998|  538.6899999999995|                            null|                             null|                   -1|                  2.08|                 561|   136.98000000000002|                 1220|                460.33|                null|                 null|                   null|                    null|                  192|                 13.54|                null|                 null|               null|                null|               null|                null|                 null|                  null|                   null|                    null|               null|                null|                     7818|                     199.6|                null|                 null|                null|                 null|                   139|                  86.18|             null|              null|                      null|                       null|                   null|                    null|                 74|  110.40000000000003|                null|                 null|                     null|                      null|             null|              null|                              null|                               null|                       27611|            7501.389999999952|                     null|                      null|\n+-----------+-----------------------+------------------------+---------------------+----------------------+---------------------+----------------------+---------------------+----------------------+--------------------+---------------------+--------------------+---------------------+-----------------------------+------------------------------+--------------------+---------------------+----------------------------+-----------------------------+---------------------+----------------------+------------------+-------------------+--------------------------------+---------------------------------+---------------------+----------------------+--------------------+---------------------+---------------------+----------------------+--------------------+---------------------+-----------------------+------------------------+---------------------+----------------------+--------------------+---------------------+-------------------+--------------------+-------------------+--------------------+---------------------+----------------------+-----------------------+------------------------+-------------------+--------------------+-------------------------+--------------------------+--------------------+---------------------+--------------------+---------------------+----------------------+-----------------------+-----------------+------------------+--------------------------+---------------------------+-----------------------+------------------------+-------------------+--------------------+--------------------+---------------------+-------------------------+--------------------------+-----------------+------------------+----------------------------------+-----------------------------------+----------------------------+-----------------------------+-------------------------+--------------------------+\n\n"}], "source": "pivoted.where(\"InvoiceDate > '2011-12-05'\").show(5)"}, {"cell_type": "code", "execution_count": 38, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+----------------------------+\n|InvoiceDate|United Kingdom_sum(Quantity)|\n+-----------+----------------------------+\n| 2011-12-06|                       27191|\n| 2011-12-05|                       42414|\n| 2011-12-09|                        9534|\n| 2011-12-02|                       24457|\n| 2011-12-08|                       32576|\n| 2011-12-07|                       27611|\n| 2011-12-04|                       10816|\n+-----------+----------------------------+\n\n"}], "source": "pivoted.where(\"InvoiceDate > '2011-12-01'\").select(\"InvoiceDate\" ,\"United Kingdom_sum(Quantity)\").show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 4}